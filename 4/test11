# -*- coding: utf-8 -*-
"""
Created on Tue Jun 21 16:59:02 2022

@author: Siavash
"""

from collections import defaultdict
from policy import e_soft_policy
from typing import Callable, Tuple




#### e_soft_policy(state:Tuple(),)



def generate_episode (env: gym.Env, Policy: Callable, max_steps: int):
    
    
    current_state=env.reset()
    
    i=0
    episod=
    cummulative_return =0
    
    while i < max_steps:
                  
        action = Policy(current_state)       
        next_state,reward,done,_ = env(current_state, action)        
        episod[i] = [current_stat,action,reward]
        current_stat = next_state
        Ap[current_stat][action]==i
        cummulative_return += reward
        i+=1 
        
        if done:
            break
        
    return episod,cummulative_return
        
        
        
        
    
def e_soft_policy (Q:difauldict, e:):
        
    if np.random < e or ???:
        policy[:] = Q[:][random]
        
        else:
            policy[:] = Q[:][argmax()]
    
    def take_action (state:Tuple(int,int),policy):     
        action = policy[state]
        
        return action
    
    return policy 



def Q4 ():
    
    
    register_env()
    # check for passing non (10,10) env's
    test_env = gym.make('FourRooms-v0', goal_pos=(3,7))
    
    env = gym.make('FourRooms-v0')


    num_episodes=
    max_steps=
    num_trials=
    gamma=

    for i in range(num_trials):
    
        Q = {} 
        N = {} 
        returnn = {}
        policy = e_soft_policy (Q,e)
    
        for j in range(num_episod):   
            
            same seed
        
            episod, cummulative_return,Ap= generate_episode(env,policy,max_steps)
            size_episod = np.shape(episod[0])
            cummulative_returns[i,j]=cummulative_return
            G=0
        
        
          
        
        
        .
        `1  
            for k in range(size_episod),-1:
            
                state,action,reward = episode(k)
            
                G = gamma*G + reward
           
                if FA:
                
                    if Ap[state][action]==k:
                    
                        returnn[state][action].append(G)            
                        N [state][action]+=1
                        Q[state][action] = sum(returnn[state][action])/N [state][action]
        
        ## plot cummulative_returns
        ## plot policy surface 
        ## plot value
        ## plot q value
        
        
if __name__ == "__main__":
    
    Q4()        
        
        
        