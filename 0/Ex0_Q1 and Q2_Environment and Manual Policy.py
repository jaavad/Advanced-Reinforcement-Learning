# -*- coding: utf-8 -*-
"""Ex0 - Manual Policy - Q1 and Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n0GjRt769mLOetHWQBH_30PxNI8Dz74e
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, Callable
from enum import IntEnum
import random

class Action(IntEnum):
    """Action"""

    LEFT = 0
    DOWN = 1
    RIGHT = 2
    UP = 3



def actions_to_dxdy(action: Action):
    """
    Helper function to map action to changes in x and y coordinates

    """
    mapping = {
        Action.LEFT: (-1, 0),
        Action.DOWN: (0, -1),
        Action.RIGHT: (1, 0),
        Action.UP: (0, 1),
    }
    return mapping[action]


def reset():
    """Return agent to start state"""
    return (0, 0)


# Q1
def simulate(state: Tuple[int, int], action: Action):
    """Simulate function for Four Rooms environment
    The general structure of this function is:
        1. If goal was reached, reset agent to start state
        2. Calculate the action taken from selected action (stochastic transition)
        3. Calculate the next state from the action taken (accounting for boundaries/walls)
        4. Calculate the reward
    """

    # Coordinate system is (x, y) where x is the horizontal and y is the vertical direction
    # I also consider the boundaries as wall
    walls = [
        (0, 5),
        (2, 5),
        (3, 5),
        (4, 5),
        (5, 0),
        (5, 2),
        (5, 3),
        (5, 4),
        (5, 5),
        (5, 6),
        (5, 7),
        (5, 9),
        (5, 10),
        (6, 4),
        (7, 4),
        (9, 4),
        (10, 4),
        (-1, 0),
        (-1, 1),
        (-1, 2),
        (-1, 3),
        (-1, 4),
        (-1, 5),
        (-1, 6),
        (-1, 7),
        (-1, 8),
        (-1, 9),
        (-1, 10),
        (11, 0),
        (11, 1),
        (11, 2),
        (11, 3),
        (11, 4),
        (11, 5),
        (11, 6),
        (11, 7),
        (11, 8),
        (11, 9),
        (11, 10),
        (0,-1),
        (1,-1),
        (2,-1),
        (3,-1),
        (4,-1),
        (5,-1),
        (6,-1),
        (7,-1),
        (8,-1),
        (9,-1),
        (10,-1),
        (0,11),
        (1,11),
        (2,11),
        (3,11),
        (4,11),
        (5,11),
        (6,11),
        (7,11),
        (8,11),
        (9,11),
        (10,11),

    ]

    
    goal_state = (10, 10) 
    
    if state == goal_state:

      next_state=reset()
      reward=0

    else:
      


      if action==Action.LEFT:
        Actions=[Action.LEFT,Action.UP,Action.DOWN]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0] 

      elif action==Action.RIGHT:
        Actions=[Action.RIGHT,Action.UP,Action.DOWN]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0]

      elif action==Action.DOWN:
        Actions=[Action.DOWN,Action.RIGHT,Action.LEFT]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0]

      elif action==Action.UP:
        Actions=[Action.UP,Action.RIGHT,Action.LEFT]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0] 
      
# calculate the next state and reward given state and action_taken
      zipped = zip(state, actions_to_dxdy(action_taken))
      mapped = map(sum, zipped)
      sums = tuple(mapped)
      next_state =  sums

# simply check whether the next state is a wall
      if next_state in walls:
        next_state=state
      else:
        pass

      if next_state == goal_state:
        reward = 1
      else :
        reward = 0

    return next_state, reward


def manual_policy(state: Tuple[int, int]):
    """A manual policy that queries user for action and returns that action

    Args:
        state (Tuple[int, int]): current agent position (e.g. (1, 3))

    Returns:
        action (Action)
    """
     # show me the state and ask me which one
    action_int = int(input("Current position is " + str(state) + ". Provide an action (0 = left, 1 = down, 2 = right, 3 = up) \n"))
    
    action = Action(action_int)

    # TODO
    return action 
    


# Q2
def agent(
    steps: int = 1000,
    trials: int = 1,
    policy=Callable[[Tuple[int, int]], Action],
):
    """
    An agent that provides actions to the environment (actions are determined by policy), and receives
    next_state and reward from the environment

    """

    cumulative_rewards = [[0 for i in range(steps)] for j in range(trials)]

    for t in range(trials):
        state = reset()
        i = 0
        rewards=0
        while i < steps:

            action = policy(state)

            
            next_state, reward=simulate(state, action)
            state=next_state
            rewards += reward
            cumulative_rewards[t][i] = rewards

            if reward == 1:
                print("Trial reward: " + str(rewards))
            i += 1
    return  cumulative_rewards




def main():

    
     steps = 50
     trials = 5

    # plot Manual Policy rewards and average
     rewards = agent(steps = steps, trials = trials, policy = manual_policy)
     average_reward = [0] * steps
     for t in range(trials):
        plt.plot(rewards[t], ':')
        for s in range(steps):
            average_reward[s] += rewards[t][s] / trials
     plt.plot(average_reward, label='Manual Policy')

     plt.xlabel("Steps")
     plt.ylabel("Cumulative Reward")
     plt.legend()
     plt.show()
pass
    
if __name__ == "__main__":
    main()

