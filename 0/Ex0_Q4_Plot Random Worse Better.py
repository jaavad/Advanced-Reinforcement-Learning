# -*- coding: utf-8 -*-
"""Ex0 - Q3 and Q4 - Plot Random_Worse_Better.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aY0wuWR7OnlpIpAaHm4kioQe1ukwwDf6
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, Callable
from enum import IntEnum
import random

class Action(IntEnum):
    """Action"""

    LEFT = 0
    DOWN = 1
    RIGHT = 2
    UP = 3



def actions_to_dxdy(action: Action):
    """
    Helper function to map action to changes in x and y coordinates

    """
    mapping = {
        Action.LEFT: (-1, 0),
        Action.DOWN: (0, -1),
        Action.RIGHT: (1, 0),
        Action.UP: (0, 1),
    }
    return mapping[action]


def reset():
    """Return agent to start state"""
    return (0, 0)


# Q1
def simulate(state: Tuple[int, int], action: Action):
    """Simulate function for Four Rooms environment
    The general structure of this function is:
        1. If goal was reached, reset agent to start state
        2. Calculate the action taken from selected action (stochastic transition)
        3. Calculate the next state from the action taken (accounting for boundaries/walls)
        4. Calculate the reward
    """

    # Coordinate system is (x, y) where x is the horizontal and y is the vertical direction
    # I also consider the boundaries as wall
    walls = [
        (0, 5),
        (2, 5),
        (3, 5),
        (4, 5),
        (5, 0),
        (5, 2),
        (5, 3),
        (5, 4),
        (5, 5),
        (5, 6),
        (5, 7),
        (5, 9),
        (5, 10),
        (6, 4),
        (7, 4),
        (9, 4),
        (10, 4),
        (-1, 0),
        (-1, 1),
        (-1, 2),
        (-1, 3),
        (-1, 4),
        (-1, 5),
        (-1, 6),
        (-1, 7),
        (-1, 8),
        (-1, 9),
        (-1, 10),
        (11, 0),
        (11, 1),
        (11, 2),
        (11, 3),
        (11, 4),
        (11, 5),
        (11, 6),
        (11, 7),
        (11, 8),
        (11, 9),
        (11, 10),
        (0,-1),
        (1,-1),
        (2,-1),
        (3,-1),
        (4,-1),
        (5,-1),
        (6,-1),
        (7,-1),
        (8,-1),
        (9,-1),
        (10,-1),
        (0,11),
        (1,11),
        (2,11),
        (3,11),
        (4,11),
        (5,11),
        (6,11),
        (7,11),
        (8,11),
        (9,11),
        (10,11),

    ]

    
    goal_state = (10, 10) 
    
    if state == goal_state:

      next_state=reset()
      reward=0

    else:
      


      if action==Action.LEFT:
        Actions=[Action.LEFT,Action.UP,Action.DOWN]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0] 

      elif action==Action.RIGHT:
        Actions=[Action.RIGHT,Action.UP,Action.DOWN]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0]

      elif action==Action.DOWN:
        Actions=[Action.DOWN,Action.RIGHT,Action.LEFT]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0]

      elif action==Action.UP:
        Actions=[Action.UP,Action.RIGHT,Action.LEFT]
        action_taken=random.choices(Actions, weights=[80,10,10], k=1)[0] 
      
# calculate the next state and reward given state and action_taken
      zipped = zip(state, actions_to_dxdy(action_taken))
      mapped = map(sum, zipped)
      sums = tuple(mapped)
      next_state =  sums

# simply check whether the next state is a wall
      if next_state in walls:
        next_state=state
      else:
        pass

      if next_state == goal_state:
        reward = 1
      else :
        reward = 0

    return next_state, reward


def agent(
    steps: int = 1000,
    trials: int = 1,
    policy=Callable[[Tuple[int, int]], Action],
):
    """
    An agent that provides actions to the environment (actions are determined by policy), and receives
    next_state and reward from the environment


    """

    cumulative_rewards = [[0 for i in range(steps)] for j in range(trials)]

    for t in range(trials):
        state = reset()
        i = 0
        rewards=0
        while i < steps:

            action = policy(state)

            action
            next_state, reward=simulate(state, action)
            state=next_state
            rewards += reward
            cumulative_rewards[t][i] = rewards

            if reward == 1:
                print("Trial reward: " + str(rewards))
            i += 1
    return  cumulative_rewards


# Q3
def random_policy(state: Tuple[int, int]):
    """A random policy that returns an action uniformly at random

    """
    action = Action(np.random.randint(4))
    return action
    


# Q4
def worse_policy(state: Tuple[int, int]):
    """A policy that is worse than the random_policy
     Just move left
    """

    return Action(3) #


 
# Q4
def better_policy(state: Tuple[int, int]):
    """A policy that is better than the random_policy

By this Policy we try to track the pass which goes toward goal point form start point 
Pass points:

    """

    if state[1] < 8: # first move up
        return Action(3)
    elif state[0] < 10: # then right
        return Action(2)
    else: # then up again
        return Action(3)
    


def main():


     steps = 10000
     trials = 10

# plot Random Policy rewards and average
     rewards = agent(steps = steps, trials = trials, policy = random_policy)
     average_reward = [0] * steps
     for t in range(trials):
        plt.plot(rewards[t], ':')
        for s in range(steps):
            average_reward[s] += rewards[t][s] / trials
        print("Random " + str(t))
     plt.plot(average_reward, label='Random Policy')
     
     
# plot Worse Policy rewards and average
     rewards = agent(steps = steps, trials = trials, policy = worse_policy)
     average_reward = [0] * steps
     for t in range(trials):
        plt.plot(rewards[t], ':')
        for s in range(steps):
            average_reward[s] += rewards[t][s] / trials
        print("Worse " + str(t))
     plt.plot(average_reward, label='Worse Policy')
     

# plot Better Policy rewards and average
     rewards = agent(steps = steps, trials = trials, policy = better_policy)
     average_reward = [0] * steps
     for t in range(trials):
        plt.plot(rewards[t], ':')
        for s in range(steps):
            average_reward[s] += rewards[t][s] / trials
        print("Better " + str(t))
     plt.plot(average_reward, label='Better Policy')
    

     plt.xlabel("Steps")
     plt.ylabel("Cumulative Reward")
     plt.legend()
     plt.show()

    
if __name__ == "__main__":
    main()

